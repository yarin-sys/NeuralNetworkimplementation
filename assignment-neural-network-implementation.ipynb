{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1882037,"sourceType":"datasetVersion","datasetId":1120859}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.141991Z","iopub.execute_input":"2025-10-02T15:38:11.142607Z","iopub.status.idle":"2025-10-02T15:38:11.160768Z","shell.execute_reply.started":"2025-10-02T15:38:11.142561Z","shell.execute_reply":"2025-10-02T15:38:11.159312Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\n","output_type":"stream"}],"execution_count":194},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nlearning_rate = 0.01                           # learning rate\nepochs = 1000                                 # max epochs\nn_hidden1 = 16                                # hidden layer 1 neurons\nn_hidden2 = 16                                 # hidden layer 2 neurons\nn_output = 1                                  # Output Layer\ninit_scale = 0.01                             # weight initialization scale\n\nactivation_choice = \"relu\"                 # \"sigmoid\", \"relu\", \"tanh\"\nGradient_decent = \"stochastic\"                # \"batch\", \"stochastic\", \"minibatch\"\nbatch_size = 32                               # only used if Gradient_decent = \"minibatch\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.162193Z","iopub.execute_input":"2025-10-02T15:38:11.162486Z","iopub.status.idle":"2025-10-02T15:38:11.179888Z","shell.execute_reply.started":"2025-10-02T15:38:11.162464Z","shell.execute_reply":"2025-10-02T15:38:11.178771Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.drop(\"id\", axis=1, inplace=True)\ndf[\"bmi\"] = df[\"bmi\"].fillna(df[\"bmi\"].mean())\n\nle = LabelEncoder()\nfor col in [\"gender\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]:\n    df[col] = le.fit_transform(df[col])\n\nX = df.drop(\"stroke\", axis=1).values\ny = df[\"stroke\"].values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.181495Z","iopub.execute_input":"2025-10-02T15:38:11.181829Z","iopub.status.idle":"2025-10-02T15:38:11.232530Z","shell.execute_reply.started":"2025-10-02T15:38:11.181807Z","shell.execute_reply":"2025-10-02T15:38:11.231537Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"#get the 2 hidden layers and output layer value from the adjustable parameter at the start \nn_input = X_train.shape[1]\nn = [n_input, n_hidden1, n_hidden2, n_output]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.233552Z","iopub.execute_input":"2025-10-02T15:38:11.233826Z","iopub.status.idle":"2025-10-02T15:38:11.239448Z","shell.execute_reply.started":"2025-10-02T15:38:11.233806Z","shell.execute_reply":"2025-10-02T15:38:11.238521Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"# Activation Functions\ndef sigmoid(z): return 1 / (1 + np.exp(-z))\ndef sigmoid_derivative(a): return a * (1 - a)\n\ndef relu(z): return np.maximum(0, z)\ndef relu_derivative(a): return (a > 0).astype(float)\n\ndef tanh(z): return np.tanh(z)\ndef tanh_derivative(a): return 1 - np.square(a)\n\nactivation_functions = {\n    \"sigmoid\": (sigmoid, sigmoid_derivative),\n    \"relu\": (relu, relu_derivative),\n    \"tanh\": (tanh, tanh_derivative)\n}\n\n# Pick an activation function for hidden layers\nact, act_deriv = activation_functions[activation_choice]\n\n# Always sigmoid at output (binary classification)\noutput_act, output_deriv = sigmoid, sigmoid_derivative\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.241358Z","iopub.execute_input":"2025-10-02T15:38:11.242129Z","iopub.status.idle":"2025-10-02T15:38:11.266371Z","shell.execute_reply.started":"2025-10-02T15:38:11.242090Z","shell.execute_reply":"2025-10-02T15:38:11.265306Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"# Initialize weights\nnp.random.seed(42)\nW1 = np.random.randn(n[1], n[0]) * init_scale\nW2 = np.random.randn(n[2], n[1]) * init_scale\nW3 = np.random.randn(n[3], n[2]) * init_scale\nb1 = np.zeros((n[1], 1))\nb2 = np.zeros((n[2], 1))\nb3 = np.zeros((n[3], 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.267270Z","iopub.execute_input":"2025-10-02T15:38:11.267593Z","iopub.status.idle":"2025-10-02T15:38:11.290790Z","shell.execute_reply.started":"2025-10-02T15:38:11.267565Z","shell.execute_reply":"2025-10-02T15:38:11.289460Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"# Loss function (binary cross-entropy)\ndef cost(y_hat, y):\n    eps = 1e-8\n    losses = - (y * np.log(y_hat + eps) + (1 - y) * np.log(1 - y_hat + eps))\n    return np.mean(losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.291863Z","iopub.execute_input":"2025-10-02T15:38:11.292135Z","iopub.status.idle":"2025-10-02T15:38:11.314213Z","shell.execute_reply.started":"2025-10-02T15:38:11.292115Z","shell.execute_reply":"2025-10-02T15:38:11.312554Z"}},"outputs":[],"execution_count":200},{"cell_type":"code","source":"# Forward Pass\ndef feed_forward(A0):\n    Z1 = W1 @ A0 + b1\n    A1 = act(Z1)\n\n    Z2 = W2 @ A1 + b2\n    A2 = act(Z2)\n\n    Z3 = W3 @ A2 + b3\n    A3 = output_act(Z3)\n\n    cache = {\"A0\": A0, \"A1\": A1, \"A2\": A2, \"A3\": A3}\n    return A3, cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.360053Z","iopub.execute_input":"2025-10-02T15:38:11.360384Z","iopub.status.idle":"2025-10-02T15:38:11.366615Z","shell.execute_reply.started":"2025-10-02T15:38:11.360363Z","shell.execute_reply":"2025-10-02T15:38:11.365744Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"# Backward Pass\ndef backprop(y_hat, Y, cache):\n    global W1, W2, W3, b1, b2, b3\n    m = Y.shape[1]\n\n    A0, A1, A2, A3 = cache[\"A0\"], cache[\"A1\"], cache[\"A2\"], cache[\"A3\"]\n\n    # Output layer\n    dZ3 = y_hat - Y\n    dW3 = (1/m) * dZ3 @ A2.T\n    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n\n    # Hidden layer 2\n    dA2 = W3.T @ dZ3\n    dZ2 = dA2 * act_deriv(A2)\n    dW2 = (1/m) * dZ2 @ A1.T\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    # Hidden layer 1\n    dA1 = W2.T @ dZ2\n    dZ1 = dA1 * act_deriv(A1)\n    dW1 = (1/m) * dZ1 @ A0.T\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2, dW3, db3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.368369Z","iopub.execute_input":"2025-10-02T15:38:11.369056Z","iopub.status.idle":"2025-10-02T15:38:11.396655Z","shell.execute_reply.started":"2025-10-02T15:38:11.369030Z","shell.execute_reply":"2025-10-02T15:38:11.395559Z"}},"outputs":[],"execution_count":202},{"cell_type":"code","source":"# Training Function\ndef train(X_train, y_train, epochs=1000, alpha=0.1):\n    global W1, W2, W3, b1, b2, b3\n    costs = []\n    A0_full = X_train.T\n    Y_full = y_train.reshape(1, -1)\n\n    for e in range(epochs):\n        if Gradient_decent == \"batch\":\n            y_hat, cache = feed_forward(A0_full)\n            dW1, db1_, dW2, db2_, dW3, db3_ = backprop(y_hat, Y_full, cache)\n\n            W1 -= alpha * dW1; b1 -= alpha * db1_\n            W2 -= alpha * dW2; b2 -= alpha * db2_\n            W3 -= alpha * dW3; b3 -= alpha * db3_\n\n        elif Gradient_decent == \"stochastic\":\n            m = A0_full.shape[1]\n            for i in range(m):\n                x_i = A0_full[:, i].reshape(-1, 1)\n                y_i = Y_full[:, i].reshape(1, 1)\n                y_hat, cache = feed_forward(x_i)\n                dW1, db1_, dW2, db2_, dW3, db3_ = backprop(y_hat, y_i, cache)\n\n                W1 -= alpha * dW1; b1 -= alpha * db1_\n                W2 -= alpha * dW2; b2 -= alpha * db2_\n                W3 -= alpha * dW3; b3 -= alpha * db3_\n\n        elif Gradient_decent == \"minibatch\":\n            m = A0_full.shape[1]\n            permutation = np.random.permutation(m)\n            A0_shuffled, Y_shuffled = A0_full[:, permutation], Y_full[:, permutation]\n\n            for i in range(0, m, batch_size):\n                X_batch = A0_shuffled[:, i:i+batch_size]\n                Y_batch = Y_shuffled[:, i:i+batch_size]\n                y_hat, cache = feed_forward(X_batch)\n                dW1, db1_, dW2, db2_, dW3, db3_ = backprop(y_hat, Y_batch, cache)\n\n                W1 -= alpha * dW1; b1 -= alpha * db1_\n                W2 -= alpha * dW2; b2 -= alpha * db2_\n                W3 -= alpha * dW3; b3 -= alpha * db3_\n\n        if e % 100 == 0:\n            y_hat, _ = feed_forward(A0_full)\n            c = cost(y_hat, Y_full)\n            print(f\"Epoch {e}: Cost = {c:.4f}\")\n            costs.append(c)\n\n    return costs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.397601Z","iopub.execute_input":"2025-10-02T15:38:11.398000Z","iopub.status.idle":"2025-10-02T15:38:11.427737Z","shell.execute_reply.started":"2025-10-02T15:38:11.397978Z","shell.execute_reply":"2025-10-02T15:38:11.426578Z"}},"outputs":[],"execution_count":203},{"cell_type":"code","source":"# Prediction\ndef predict(X):\n    A0 = X.T\n    y_hat, _ = feed_forward(A0)\n    return (y_hat > 0.5).astype(int)\n\ndef accuracy(y_hat, y_true):\n    return np.mean(y_hat.reshape(-1) == y_true) * 100\n\n# =========================\n# Run Training\n# =========================\ncosts = train(X_train, y_train, epochs=epochs, alpha=learning_rate)\n\n# Evaluate\ny_pred = predict(X_test)\nacc = accuracy(y_pred, y_test)\nprint(\"Test Accuracy:\", acc, \"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T15:38:11.429650Z","iopub.execute_input":"2025-10-02T15:38:11.430022Z","iopub.status.idle":"2025-10-02T15:45:54.033572Z","shell.execute_reply.started":"2025-10-02T15:38:11.429997Z","shell.execute_reply":"2025-10-02T15:45:54.032704Z"}},"outputs":[{"name":"stdout","text":"Epoch 0: Cost = 0.1858\nEpoch 100: Cost = 0.1487\nEpoch 200: Cost = 0.1468\nEpoch 300: Cost = 0.1421\nEpoch 400: Cost = 0.1369\nEpoch 500: Cost = 0.1335\nEpoch 600: Cost = 0.1302\nEpoch 700: Cost = 0.1269\nEpoch 800: Cost = 0.1225\nEpoch 900: Cost = 0.1180\nTest Accuracy: 92.75929549902152 %\n","output_type":"stream"}],"execution_count":204}]}